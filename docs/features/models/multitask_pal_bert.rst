Multi-task PAL BERT in DeepPavlov
=================================

Multi-task PAL BERT in DeepPavlov is an implementation of BERT training algorithm published in the paper "BERT and PALs: Projected Attention Layers for
Efficient Adaptation in Multi-Task Learning".

| Multitask BERT and PALs paper: https://arxiv.org/pdf/1902.02671.pdf

The idea is to share BERT body between several tasks. This is necessary if a model pipe has several
components using BERT and the amount of GPU memory is limited. Each task has its own 'classifier' part attached to the
output of the BERT encoder. If multi-task BERT has :math:`T` heads, one training iteration consists of

- composing :math:`T` mini-batches, one for the task to be trained on(as specified by the task ID) and rest are  dummy/sample batches,

- :math:`n` gradient step as provided in gradient_accumulation_steps (1 by default), for the tasks specified by task ID.

When one of BERT heads is being trained, other heads' parameters do not change. On each training step both BERT head
and body parameters are modified.

At this page, multi-task PAL BERT usage is explained on a toy configuration file of a model that detects
insults(for demonstration we will use the data for both the tasks).

We start with the ``metadata`` field of the configuration file. Multi-task PAL BERT model is saved in
``{"MODELS_PATH": "{ROOT_PATH}/models"}``. ``downloads``
field of Multitask PAL BERT configuration file is a union of ``downloads`` fields of original configs without pre-trained
models. The ``metadata`` field of our config is given below.

.. code:: json

    {
      "metadata": {
        "variables": {
          "ROOT_PATH": "~/.deeppavlov",
          "DOWNLOADS_PATH": "{ROOT_PATH}/downloads",
          "PRETRAINED_BERT": "{ROOT_PATH}/pretrained_bert",
          "MODELS_PATH": "{ROOT_PATH}/models"
        },
        "download": [
          {
            "url": "http://files.deeppavlov.ai/datasets/insults_data.tar.gz",
            "subdir": "{DOWNLOADS_PATH}"
          },
          {
            "url": "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin",
            "subdir": "{PRETRAINED_BERT}"
          }
        ]
      }
    }

Train config
------------

Data reading and iteration is performed by ``multitask_reader`` and ``multitask_pal_bert_iterator``. These classes are composed
of task readers and iterators and generate batches that contain data from heterogeneous datasets.

A ``multitask_reader`` configuration has parameters ``class_name``, ``data_path``, and ``tasks``.
``data_path`` field may be any string because data paths are passed for tasks individually in ``tasks``
parameter. However, you can not drop a ``data_path`` parameter because it is obligatory for dataset reader
configuration. ``tasks`` parameter is a dictionary of task dataset readers configurations. In configurations of
task readers, ``reader_class_name`` parameter is used instead of ``class_name``. The dataset reader configuration is
provided:

.. code:: json

    {
      "dataset_reader": {
        "class_name": "multitask_reader",
        "data_path": "null",
        "tasks": {
          "insults": {
            "reader_class_name": "basic_classification_reader",
            "x": "Comment",
            "y": "Class",
            "data_path": "{DOWNLOADS_PATH}/insults_data"
          },
          "insults1": {
            "reader_class_name": "basic_classification_reader",
            "x": "Comment",
            "y": "Class",
            "data_path": "{DOWNLOADS_PATH}/insults_data"
          }
        }
      },
    }

A ``multitask_pal_bertiterator`` configuration has parameters ``num_train_epochs``, ``steps_per_epoch``, ``class_name`` 
and ``tasks``. ``tasks`` is a dictionary of configurations of task iterators. In configurations of task iterators, 
``iterator_class_name`` is used instead of``class_name``. Also provide ``gradient_accumulation_steps`` if using gradient
accumulation. The dataset iterator configuration is as follows:

.. code:: json

    {
      "dataset_iterator": {
        "class_name": "multitask_pal_bert_iterator",
        "num_train_epochs": 5,
        "steps_per_epoch": 100,
        "tasks": {
          "insults": {
            "iterator_class_name": "basic_classification_iterator",
            "seed": 42
          },
          "insults1": {
            "iterator_class_name": "basic_classification_iterator",
            "seed": 42
          }
        }
    }

Batches generated by ``multitask_iterator`` are tuples of two elements: inputs of the model and labels. Both inputs
and labels are lists of tuples. The inputs have following format: ``[(first_task_inputs[0], second_task_inputs[0],
...), (first_task_inputs[1], second_task_inputs[1], ...), ...]`` where ``first_task_inputs``, ``second_task_inputs``,
and so on are x values of batches from task dataset iterators. The labels in the have the similar format. Along with 
that inputs have task ID along with them which will be extracted later using the ``pal_bert_preprocessor``. 

In this tutorial, there are 2 datasets. Considering the batch structure, ``chainer`` inputs are:

.. code:: json

    {
      "in": ["x_insults1_with_id", "x_insults2_with_id"],
      "in_y": ["y_insults1", "y_insults2"]
    }

To extract the task id from the inputs we need to use the component ``pal_bert_preprocessor`` which has parameters
``class_name``, ``in`` and ``out``. The first valiable out will always be the task id and make sure the relative order
for the task inputs is the same.

.. code:: json

    {
          "class_name": "multitask_pal_bert_preprocessor",
          "in": ["x_insults1_with_id", "x_insults2_with_id"],
          "out": ["task_id", "x_insults", "x_insults2"]
    },

Sometimes a task dataset iterator returns inputs or labels consisting of more than one element. For example, in model
:config:`mt_bert_train_tutorial.json <kbqa/kbqa_mt_bert_train.json>` ``siamese_iterator`` input
element consists of 2 strings. If there is a necessity to split such a variable, ``InputSplitter`` component can
be used.

Data preparation steps in the pipe of tutorial config are similar to data preparation steps in the original
configs except for names of the variables.

A ``multitask_pal_bert`` component has task-specific parameters and parameters that are common for all tasks. The first
are provided inside the ``tasks`` parameter. The ``tasks`` is a dictionary that keys are task names and values are 
task-specific parameters.


Inputs and labels of a ``multitask_pal_bert`` component are distributed between
the tasks according to the ``in_distribution`` and ``in_y_distribution`` parameters. 
First, ``in`` and ``in_y`` elements have to be grouped by tasks and the first parameter of ``in`` should be the task id
extracted by the ``multitask_pal_bert_preprocessor`` followed by the input for each task specified, e.g. arguments for the
first task, then arguments for the second task and so on. Secondly, the order of tasks in ``in`` and ``in_y`` has to
be the same as the order of tasks in the ``in_distribution`` and ``in_y_distribution`` parameters. If ``in`` and ``in_y`` 
parameters are dictionaries, you may make ``in_distribution`` and ``in_y_distribution`` parameter dictionaries which keys 
are task names and values are lists of elements of ``in`` or ``in_y``. If usinggradient accumulation also provide the 
``gradient_accumulation_steps`` and ``steps_per_epoch`` parameters.

.. code:: json

    {
        "id": "multitask_pal_bert",
        "class_name": "multitask_pal_bert",
        "pretrained_bert": "{PRETRAINED_BERT}/pytorch_model.bin",
        "optimizer_parameters": {"lr": 3e-5},
        "learning_rate_drop_patience": 2,
        "learning_rate_drop_div": 2.0,
        "return_probas": true,
        "save_path": "{MODELS_PATH}/model",
        "load_path": "{MODELS_PATH}/model",
        "tasks": {
            "insults1": {
                "n_classes": "#vocab_insults1.len"
            },
            "insults2": {
                "n_classes": "#vocab_insults2.len"
            }
        },
        "in_distribution": {
            "insults1": 1,
            "insults2": 1
        },
        "in": [
            "task_id",
            "bert_features_insults1",
            "bert_features_insults2"
        ],
        "in_y_distribution": {
            "insults1": 1,
            "insults2": 1
        },
        "in_y": [
            "y_ids_insults1",
            "y_ids_insults2"
        ],
        "out": [
            "y_insults1_pred_probas",
            "y_insults2_pred_probas"
        ]
    }
