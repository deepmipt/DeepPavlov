{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-shot Text Classification\n",
    "\n",
    "# Table of contents \n",
    "\n",
    "1. [Introduction to the task](#1.-Introduction-to-the-task)\n",
    "\n",
    "2. [Dataset](#2.-Datasets)\n",
    "    \n",
    "    2.1. [Datasets format](#2.1-Datasets-format)\n",
    "\n",
    "3. [Model architecture](#3.-Model-architecture)\n",
    "\n",
    "4. [Get started with the model](#4.-Get-started-with-the-model)\n",
    "    \n",
    "    4.1. [Installation](#4.1-Installation)\n",
    "\n",
    "    4.2. [Support dataset configuration](#4.2-Support-dataset-configuration)\n",
    "\n",
    "5. [Use the model for prediction](#5.-Use-the-model-for-prediction)\n",
    "\n",
    "    5.1. [Predict using Python](#5.1-Predict-using-Python)\n",
    "    \n",
    "    5.2. [Predict using CLI](#5.2-Predict-using-CLI)\n",
    "      \n",
    "6. [Evaluate](#6.-Evaluate)\n",
    "    \n",
    "    6.1. [Evaluate from Python](#6.1-Evaluate-from-Python)\n",
    "    \n",
    "    6.2. [Evaluate from CLI](#6.2-Evaluate-from-CLI)\n",
    "\n",
    "# 1. Introduction to the task\n",
    "\n",
    "__Text classification__ is a task of identifying one of the pre-defined label given an utterance, where label is one of N classes or \"OOS\" (out-of-scope examples - utterances that do not belong to any of the predefined classes). We consider few-shot setting, where only few examples (5 or 10) per intent class are given as a training set.\n",
    "\n",
    "# 2. Dataset\n",
    "\n",
    "In our experiments we used the [CLINC150](https://paperswithcode.com/dataset/clinc150) dataset, which has 10 different domains with 15 intents each, 100 shots per intent class and 1000 OOS examples. It simulates a setting, where model has to handle many different services with wide variety of intents.\n",
    "\n",
    "Specifically, we validate our model on CLINC150 from the original DNNC paper. We parsed it to match the format described [below](#2.1-Datasets-format). The original dataset can be downloaded from the DNNC [github page](https://github.com/salesforce/DNNC-few-shot-intent).\n",
    "\n",
    "## 2.1 Datasets format\n",
    "\n",
    "Train, dev and test set are separate json files, which have the following format\n",
    "\n",
    "```\n",
    "{\n",
    "    \"columns\": [\n",
    "        \"text\",\n",
    "        \"category\"\n",
    "    ],\n",
    "\n",
    "    \"data\": [\n",
    "\n",
    "        [\n",
    "            \"text\"\n",
    "            \"label\"\n",
    "        ],\n",
    "\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "# 3. Model architecture\n",
    "\n",
    "The typical methodology of few-shot text classification is to embed each example into a vector space and use an off-the-shelf distance metric to perform a similarity search. However, the text embedding methods do not discriminate the OOS examples well enough.\n",
    "\n",
    "\n",
    "DNNC authors suggests to model fine-grained relations of utterance pairs via pairise simmilarity:\n",
    "\n",
    "$h = BERT([[CLS], u, [SEP], e_{j,i}, [SEP]]) \\in \\R^d$\n",
    "\n",
    "$S(u, e_{j,i}) = \\sigma(W * h + b) \\in \\R$, where $e_{j, i} \\in E $- training set, $W \\in \\R^{1×d}$, $b \\in \\R$\n",
    "\n",
    "To mitigate the data scarcity setting in few-shot learning, DNNC uses knowldge-transfer from NLI task. We pretrain [roberta-base](https://huggingface.co/roberta-base) on combination of 3 NLI datasets: SNLI, WNLI, MNLI.\n",
    "\n",
    "# 4. Get started with the model\n",
    "\n",
    "## 4.1 Installation\n",
    "\n",
    "First make sure you have the DeepPavlov Library installed.\n",
    "[More info about the first installation.](http://docs.deeppavlov.ai/en/master/intro/installation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make sure that all the required packages, datasets and weights are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov install dnnc_infer\n",
    "!python -m deeppavlov download dnnc_infer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dnnc_infer` is the name of the model's *config_file*. [What is a Config File?](http://docs.deeppavlov.ai/en/master/intro/configuration.html) \n",
    "\n",
    "Configuration file defines the model and describes it's hyperparameters\n",
    "\n",
    "## 4.2 Support dataset configuration\n",
    "\n",
    "Before making predictions or evaluation you need to set path to your support dataset. DNNC model compares input text to every example in support dataset to determine, which class the input example belongs to. By default, the model uses training set as support dataset. It is automatically saved by *dataset_iterator*  during the training step, but you can specify your own support dataset in the in `dnnc_infer` config file. It has the same format as metioned [before](#2.1-Datasets-format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.deeppavlov/parsed_datasets/parsed_dataset.json\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.commands.utils import parse_config\n",
    "\n",
    "model_config = parse_config('dnnc_infer')\n",
    "\n",
    "#  dataset for predictions\n",
    "print(model_config['chainer']['pipe'][0]['support_dataset_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-the-shelf prediction\n",
    "\n",
    "Base model was already pre-trained to recognize simmilar utterances, so you can use off-the-shelf model to make predictions and evalutation. No additional training needed.\n",
    "\n",
    "### OOS prediction\n",
    "\n",
    "Out-of-scope (OOS) examples are determined via confidence_threshold parameter with the following algorithm. Firstly model calculates an average similarity score for every class from support dataset. Secondly it determines the class with maximum similarity score. Finally the model predicts class with maximum similarity if it's score is higher than confidence_threshold and \"oos\" class otherwise. The higher the threshold, the more often the model predicts \"oos\" class. By default it is set to 0.5 and you can change it to your preferences in configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.commands.utils import parse_config\n",
    "\n",
    "model_config = parse_config('dnnc_infer')\n",
    "\n",
    "#  dataset for predictions\n",
    "print(model_config['chainer']['pipe'][-1]['confidence_threshold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Use the model for prediction\n",
    "\n",
    "## 5.1 Predict using Python\n",
    "\n",
    "After [installing](#4.-Get-started-with-the-model) the model, build it from the config and predict. If you set 'download' flag to 'True', then existing model weights will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book_hotel', 'international_visa']\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model, configs\n",
    "\n",
    "model = build_model(\"dnnc_infer\", install=True, download=True)\n",
    "\n",
    "model([\"can you find me a good reviewed hotel in japan\", \"if i get a visa can i travel to japan\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Predict using CLI\n",
    "\n",
    "You can also get predictions in an interactive mode through CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov interact dnnc_infer [-d] [-i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or make predictions for samples from *stdin*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov predict dnnc_infer [-d] [-i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate\n",
    "\n",
    "To evaluate the model on your data, you need to change the path to the dataset in `dnnc_infer` config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.deeppavlov/downloads/clinc150\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.commands.utils import parse_config\n",
    "\n",
    "model_config = parse_config('dnnc_infer')\n",
    "\n",
    "#  dataset for evaluation\n",
    "print(model_config['dataset_reader']['data_path'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Evaluate from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import evaluate_model\n",
    "\n",
    "model = evaluate_model('dnnc_infer', install=True, download=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Evaluate from CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov evaluate dnnc_infer [-d] [-i]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
