{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzGA9tNRdPKd"
   },
   "source": [
    "<center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepmipt/DeepPavlov/blob/master/examples/mt_pal_bert_tutorial_mrpc_rte.ipynb)\n",
    "\n",
    "![Alt Text](https://static.tildacdn.com/tild3762-3666-4530-b139-666433343863/_DeepPavlov_-5.png)\n",
    "\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "# **Multi-Task Pal Bert in DeepPavlov**\n",
    "\n",
    "The DeepPavlov Library consists of a lot of state of the art NLP techniques and Multi-task BERT is one of them.\n",
    "\n",
    "Multi-task learning shares information between related tasks, reducing the number of parameters required. State of the art results across natural language understanding tasks in the GLUE benchmark has been previously used transfer learning from a large task: unsupervised training with BERT, where a separate BERT model was fine-tuned for each task.\n",
    "\n",
    "In multi-task BERT we share a single BERT model along with a small number of task-specific parameters and match the performance of separately fine-tuned models with fewer parameters on the GLUE benchmark.\n",
    "\n",
    "The Multi-Task Pal Bert model is based on the Bert-n-Pals paper: [arxiv.org/pdf/1902.02671.pdf](https://arxiv.org/pdf/1902.02671.pdf)\n",
    "\n",
    "\n",
    " This model uses the additional task specific **Projected Attention Layers or PALs** in parallel with self attention layers.\n",
    "\n",
    "\n",
    "Along with using these additional layers, during training the, rather than training sequentially on each task, it picks the task to be trained on by making a random based on some provided *probs*. These *probs* are directly propotional to the *size of the training data*.\n",
    "\n",
    "\n",
    "- `List of Probabilities = [List of probabilities for each task (proportional to train size)]`\n",
    "\n",
    "- `Task_id = np.random.choice(number_tasks, List of Probabilities)`\n",
    "\n",
    "- `Train only on the batch of that Task_id`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Using Multi-Task Bert we can achieve better results while using less memory.\n",
    "To use multitask pal bert we need to make use that we need to use **4 basic components**, which are:\n",
    "\n",
    "\n",
    "- `multitask_reader`\n",
    "\n",
    "- `multitask_pal_iterator`\n",
    "\n",
    "- `multitask_pal_bert_preprocessor`\n",
    "\n",
    "- `multitask_pal_bert`\n",
    "\n",
    "In this tutorial we will train a multitask model on two GLUE Benchmark datasets MRPC and RTE, you can read more about the GULE Benchmark here: [gluebenchmark.com](https://gluebenchmark.com/) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdMaTgFTq_fU"
   },
   "source": [
    "## Download the datasets\n",
    "\n",
    "First we need to download the data locally and unzip it in their respective folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xVCY72tyLs9v",
    "outputId": "de612009-927c-4509-cfd3-fc76960fa096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-19 11:33:15--  https://dl.fbaipublicfiles.com/glue/data/RTE.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 697150 (681K) [application/zip]\n",
      "Saving to: ‘RTE.zip’\n",
      "\n",
      "RTE.zip             100%[===================>] 680.81K   664KB/s    in 1.0s    \n",
      "\n",
      "2021-08-19 11:33:17 (664 KB/s) - ‘RTE.zip’ saved [697150/697150]\n",
      "\n",
      "Archive:  RTE.zip\n",
      "   creating: RTE/\n",
      "  inflating: RTE/dev.tsv             \n",
      "  inflating: RTE/test.tsv            \n",
      "  inflating: RTE/train.tsv           \n",
      "--2021-08-19 11:33:17--  https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1047044 (1023K) [text/plain]\n",
      "Saving to: ‘/content/MRPC/msr_paraphrase_train.txt’\n",
      "\n",
      "msr_paraphrase_trai 100%[===================>]   1023K   831KB/s    in 1.2s    \n",
      "\n",
      "2021-08-19 11:33:19 (831 KB/s) - ‘/content/MRPC/msr_paraphrase_train.txt’ saved [1047044/1047044]\n",
      "\n",
      "--2021-08-19 11:33:19--  https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 441275 (431K) [text/plain]\n",
      "Saving to: ‘/content/MRPC/msr_paraphrase_test.txt’\n",
      "\n",
      "msr_paraphrase_test 100%[===================>] 430.93K   521KB/s    in 0.8s    \n",
      "\n",
      "2021-08-19 11:33:21 (521 KB/s) - ‘/content/MRPC/msr_paraphrase_test.txt’ saved [441275/441275]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://dl.fbaipublicfiles.com/glue/data/RTE.zip && unzip RTE.zip\n",
    "! wget https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt -P /content/MRPC\n",
    "! wget https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt -P /content/MRPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3rVwEgtdM8v"
   },
   "source": [
    "## Explore the Data\n",
    "\n",
    "Next we explore the data, we open the tsv files for RTE using Pandas. \n",
    "\n",
    "**NOTE** - *Quoting* should be set to `3` to avoid errors while reading the *train.tsv* and *dev.tsv*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "HOiaNBsyfAlu",
    "outputId": "a2224598-0aba-4af8-f0ad-689fb673aca9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>No Weapons of Mass Destruction Found in Iraq Yet.</td>\n",
       "      <td>Weapons of Mass Destruction Found in Iraq.</td>\n",
       "      <td>not_entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A place of sorrow, after Pope John Paul II die...</td>\n",
       "      <td>Pope Benedict XVI is the new leader of the Rom...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Herceptin was already approved to treat the si...</td>\n",
       "      <td>Herceptin can be used to treat breast cancer.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Judie Vivian, chief executive at ProMedica, a ...</td>\n",
       "      <td>The previous name of Ho Chi Minh City was Saigon.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A man is due in court later charged with the m...</td>\n",
       "      <td>Paul Stewart Hutchinson is accused of having s...</td>\n",
       "      <td>not_entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  ...           label\n",
       "0      0  ...  not_entailment\n",
       "1      1  ...      entailment\n",
       "2      2  ...      entailment\n",
       "3      3  ...      entailment\n",
       "4      4  ...  not_entailment\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train  = pd.read_csv(\"/content/RTE/train.tsv\", sep=\"\\t\", quoting=3)\n",
    "df_valid  = pd.read_csv(\"/content/RTE/dev.tsv\", sep=\"\\t\", quoting=3)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJeB94-Vs-D8"
   },
   "source": [
    "We can see that this is a sentance pair entailment prediction task.\n",
    "\n",
    "Next we create a new directory `glue_csv` and within that two more directories for the task data where we will save the train and valid data(dev.tsv) in CSV format as we will be using the `basic_classification_reader` to read the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPtDse-Bus_e"
   },
   "source": [
    "## Make the new directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z2RY36yTfNlt"
   },
   "outputs": [],
   "source": [
    "! mkdir /content/glue_csv /content/glue_csv/RTE /content/glue_csv/MRPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_gJdmTsuzZP"
   },
   "source": [
    "Save the data in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lyPrTLuPgYsN"
   },
   "outputs": [],
   "source": [
    "df_train.to_csv(\"glue_csv/RTE/train.csv\")\n",
    "df_valid.to_csv(\"glue_csv/RTE/valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiH9ZV_kvBON"
   },
   "source": [
    "We will do the same with with the data for MRPC, if you want you can also explore this data by following the same setps as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mn2gZWtwhQFt"
   },
   "outputs": [],
   "source": [
    "df_train  = pd.read_csv(\"/content/MRPC/msr_paraphrase_train.txt\", sep=\"\\t\", quoting=3)\n",
    "df_valid  = pd.read_csv(\"/content/MRPC/msr_paraphrase_test.txt\", sep=\"\\t\", quoting=3)\n",
    "df_train.to_csv(\"/content/glue_csv/MRPC/train.csv\")\n",
    "df_valid.to_csv(\"/content/glue_csv/MRPC/valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IixLhsWG0ZUf"
   },
   "source": [
    "## Install DeepPavlov Using pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iBVb7ICi0MQW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/rimijoker/DeepPavlov.git@pal-bert\n",
      "  Cloning https://github.com/rimijoker/DeepPavlov.git (to revision pal-bert) to /tmp/pip-req-build-shr0ivf9\n",
      "  Running command git clone -q https://github.com/rimijoker/DeepPavlov.git /tmp/pip-req-build-shr0ivf9\n",
      "  Running command git checkout -b pal-bert --track origin/pal-bert\n",
      "  Switched to a new branch 'pal-bert'\n",
      "  Branch 'pal-bert' set up to track remote branch 'pal-bert' from 'origin'.\n",
      "Collecting aio-pika==6.4.1\n",
      "  Downloading aio_pika-6.4.1-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 21 kB/s \n",
      "\u001b[?25hCollecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 6.9 MB/s \n",
      "\u001b[?25hCollecting fastapi==0.47.1\n",
      "  Downloading fastapi-0.47.1-py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from deeppavlov==0.16.0) (3.0.12)\n",
      "Collecting h5py==2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 38.5 MB/s \n",
      "\u001b[?25hCollecting nltk==3.4.5\n",
      "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 54.7 MB/s \n",
      "\u001b[?25hCollecting numpy==1.18.0\n",
      "  Downloading numpy-1.18.0-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1 MB 1.5 MB/s \n",
      "\u001b[?25hCollecting overrides==2.7.0\n",
      "  Downloading overrides-2.7.0.tar.gz (4.5 kB)\n",
      "Collecting pandas==0.25.3\n",
      "  Downloading pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4 MB 42.3 MB/s \n",
      "\u001b[?25hCollecting prometheus-client==0.7.1\n",
      "  Downloading prometheus_client-0.7.1.tar.gz (38 kB)\n",
      "Collecting pytz==2019.1\n",
      "  Downloading pytz-2019.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 47.6 MB/s \n",
      "\u001b[?25hCollecting pydantic==1.3\n",
      "  Downloading pydantic-1.3-cp37-cp37m-manylinux2010_x86_64.whl (7.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.3 MB 50.5 MB/s \n",
      "\u001b[?25hCollecting pymorphy2==0.8\n",
      "  Downloading pymorphy2-0.8-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 4.1 MB/s \n",
      "\u001b[?25hCollecting pymorphy2-dicts-ru\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.2 MB 42.6 MB/s \n",
      "\u001b[?25hCollecting pyopenssl==19.1.0\n",
      "  Downloading pyOpenSSL-19.1.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 2.4 MB/s \n",
      "\u001b[?25hCollecting pytelegrambotapi==3.6.7\n",
      "  Downloading pyTelegramBotAPI-3.6.7.tar.gz (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 4.0 MB/s \n",
      "\u001b[?25hCollecting requests==2.22.0\n",
      "  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 5.9 MB/s \n",
      "\u001b[?25hCollecting ruamel.yaml==0.15.100\n",
      "  Downloading ruamel.yaml-0.15.100-cp37-cp37m-manylinux1_x86_64.whl (654 kB)\n",
      "\u001b[K     |████████████████████████████████| 654 kB 45.3 MB/s \n",
      "\u001b[?25hCollecting rusenttokenize==0.0.5\n",
      "  Downloading rusenttokenize-0.0.5-py3-none-any.whl (10 kB)\n",
      "Collecting scikit-learn==0.21.2\n",
      "  Downloading scikit_learn-0.21.2-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.7 MB 54.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov==0.16.0) (1.4.1)\n",
      "Collecting tqdm==4.41.1\n",
      "  Downloading tqdm-4.41.1-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 4.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from deeppavlov==0.16.0) (7.1.2)\n",
      "Collecting uvicorn==0.11.7\n",
      "  Downloading uvicorn-0.11.7-py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
      "\u001b[?25hCollecting sacremoses==0.0.35\n",
      "  Downloading sacremoses-0.0.35.tar.gz (859 kB)\n",
      "\u001b[K     |████████████████████████████████| 859 kB 16.9 MB/s \n",
      "\u001b[?25hCollecting uvloop==0.14.0\n",
      "  Downloading uvloop-0.14.0-cp37-cp37m-manylinux2010_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 41.1 MB/s \n",
      "\u001b[?25hCollecting yarl\n",
      "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 48.9 MB/s \n",
      "\u001b[?25hCollecting aiormq<4,>=3.2.0\n",
      "  Downloading aiormq-3.3.1-py3-none-any.whl (28 kB)\n",
      "Collecting starlette<=0.12.9,>=0.12.9\n",
      "  Downloading starlette-0.12.9.tar.gz (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 3.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0->deeppavlov==0.16.0) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->deeppavlov==0.16.0) (2.8.2)\n",
      "Collecting dawg-python>=0.7\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 20.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov==0.16.0) (0.6.2)\n",
      "Collecting cryptography>=2.8\n",
      "  Downloading cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 39.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov==0.16.0) (1.24.3)\n",
      "Collecting idna<2.9,>=2.5\n",
      "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 6.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov==0.16.0) (2021.5.30)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov==0.16.0) (3.0.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->deeppavlov==0.16.0) (1.0.1)\n",
      "Collecting websockets==8.*\n",
      "  Downloading websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 8.2 MB/s \n",
      "\u001b[?25hCollecting h11<0.10,>=0.8\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 2.2 MB/s \n",
      "\u001b[?25hCollecting httptools==0.1.*\n",
      "  Downloading httptools-0.1.2-cp37-cp37m-manylinux1_x86_64.whl (219 kB)\n",
      "\u001b[K     |████████████████████████████████| 219 kB 54.7 MB/s \n",
      "\u001b[?25hCollecting pamqp==2.3.0\n",
      "  Downloading pamqp-2.3.0-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov==0.16.0) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov==0.16.0) (2.20)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov==0.16.0) (3.7.4.3)\n",
      "Collecting multidict>=4.0\n",
      "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[K     |████████████████████████████████| 142 kB 50.1 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: deeppavlov, nltk, overrides, prometheus-client, pytelegrambotapi, sacremoses, starlette\n",
      "  Building wheel for deeppavlov (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for deeppavlov: filename=deeppavlov-0.16.0-py3-none-any.whl size=933793 sha256=69859571597ff97b8687d22195fc3e1781f574e62b4f16de0a94160c2382b6d8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xz_6c61y/wheels/3c/3f/e2/84de308ff50a69ddfd9a707c801ae365bf7ea2742ff766a5fb\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449921 sha256=f33359545373c9f3e25ad8e1ac0124d9b992275bac445b367572ada9d93359b7\n",
      "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for overrides: filename=overrides-2.7.0-py3-none-any.whl size=5605 sha256=a37d63cb942a61e9beab63a6da25e6e52758123188873c6f35f7b3056ee5dc01\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/87/45/bfdacf6c3b8233b6e8d519edcbd1cf297ad5ff5f0bf84bb9c1\n",
      "  Building wheel for prometheus-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for prometheus-client: filename=prometheus_client-0.7.1-py3-none-any.whl size=41404 sha256=a6b429db821f8c5aeb759845bbf70f3408e21c23d18061c15baad0fd7a48d4f8\n",
      "  Stored in directory: /root/.cache/pip/wheels/30/0c/26/59ba285bf65dc79d195e9b25e2ddde4c61070422729b0cd914\n",
      "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-py3-none-any.whl size=47176 sha256=b3454ce70793e856c4cf4fd6de90a398907badca1826b0b0f7ce700d9e39d3a2\n",
      "  Stored in directory: /root/.cache/pip/wheels/7f/7c/54/8eddf2369ef1b9190e2ee6dc2b40df54b6c65529a38790fdd4\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.35-py3-none-any.whl size=883989 sha256=b25b3b7689ee61f6b6ce1072113261330a0415efe55412a2c67135be98a498c5\n",
      "  Stored in directory: /root/.cache/pip/wheels/d1/ff/0e/e00ff1e22100702ac8b24e709551ae0fb29db9ffc843510a64\n",
      "  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for starlette: filename=starlette-0.12.9-py3-none-any.whl size=57251 sha256=7c20001b2fa5dac937882299719cbce033ece33817f508d87fc82f80d41403c2\n",
      "  Stored in directory: /root/.cache/pip/wheels/e8/78/be/f57ed5aed7cd222abdb24e3186b5c9f1074184fcc0a295102b\n",
      "Successfully built deeppavlov nltk overrides prometheus-client pytelegrambotapi sacremoses starlette\n",
      "Installing collected packages: multidict, idna, yarl, pamqp, numpy, websockets, uvloop, tqdm, starlette, requests, pytz, pymorphy2-dicts, pydantic, httptools, h11, dawg-python, cryptography, aiormq, uvicorn, scikit-learn, sacremoses, rusenttokenize, ruamel.yaml, pytelegrambotapi, pyopenssl, pymorphy2-dicts-ru, pymorphy2, prometheus-client, pandas, overrides, nltk, h5py, fastapi, Cython, aio-pika, deeppavlov\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 2.10\n",
      "    Uninstalling idna-2.10:\n",
      "      Successfully uninstalled idna-2.10\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.62.0\n",
      "    Uninstalling tqdm-4.62.0:\n",
      "      Successfully uninstalled tqdm-4.62.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.23.0\n",
      "    Uninstalling requests-2.23.0:\n",
      "      Successfully uninstalled requests-2.23.0\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2018.9\n",
      "    Uninstalling pytz-2018.9:\n",
      "      Successfully uninstalled pytz-2018.9\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.2.post1\n",
      "    Uninstalling scikit-learn-0.22.2.post1:\n",
      "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
      "  Attempting uninstall: prometheus-client\n",
      "    Found existing installation: prometheus-client 0.11.0\n",
      "    Uninstalling prometheus-client-0.11.0:\n",
      "      Successfully uninstalled prometheus-client-0.11.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.5\n",
      "    Uninstalling pandas-1.1.5:\n",
      "      Successfully uninstalled pandas-1.1.5\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.24\n",
      "    Uninstalling Cython-0.29.24:\n",
      "      Successfully uninstalled Cython-0.29.24\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xarray 0.18.2 requires pandas>=1.0, but you have pandas 0.25.3 which is incompatible.\n",
      "tensorflow 2.6.0 requires h5py~=3.1.0, but you have h5py 2.10.0 which is incompatible.\n",
      "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.18.0 which is incompatible.\n",
      "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.1 which is incompatible.\n",
      "kapre 0.3.5 requires numpy>=1.18.5, but you have numpy 1.18.0 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 0.25.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.22.0 which is incompatible.\n",
      "fbprophet 0.7.1 requires pandas>=1.0.4, but you have pandas 0.25.3 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.3.1 cryptography-3.4.7 dawg-python-0.7.2 deeppavlov-0.16.0 fastapi-0.47.1 h11-0.9.0 h5py-2.10.0 httptools-0.1.2 idna-2.8 multidict-5.1.0 nltk-3.4.5 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pandas-0.25.3 prometheus-client-0.7.1 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.417127.4579844 pyopenssl-19.1.0 pytelegrambotapi-3.6.7 pytz-2019.1 requests-2.22.0 ruamel.yaml-0.15.100 rusenttokenize-0.0.5 sacremoses-0.0.35 scikit-learn-0.21.2 starlette-0.12.9 tqdm-4.41.1 uvicorn-0.11.7 uvloop-0.14.0 websockets-8.1 yarl-1.6.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy",
         "pandas",
         "pytz"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "! pip install deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dHaGaY92GT0"
   },
   "source": [
    "### Now lets us Explore the config for this tutorial\n",
    "\n",
    "For this tutorial we will train a multitask model on two tasks MRPC and RTE, there is already a config file to make things easier, lets explore the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvgK5uR6UdmV",
    "outputId": "cb2f196d-94e4-469d-d7af-c6b99016df87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chainer': {'in': ['x_mrpc_with_id', 'x_rte_with_id'],\n",
      "             'in_y': ['y_mrpc', 'y_rte'],\n",
      "             'out': ['y_mrpc_pred_labels', 'y_rte_pred_labels'],\n",
      "             'pipe': [{'class_name': 'multitask_pal_bert_preprocessor',\n",
      "                       'in': ['x_mrpc_with_id', 'x_rte_with_id'],\n",
      "                       'out': ['task_id', 'x_mrpc', 'x_rte']},\n",
      "                      {'class_name': 'input_splitter',\n",
      "                       'in': ['x_mrpc'],\n",
      "                       'keys_to_extract': [0, 1],\n",
      "                       'out': ['x_mrpc1', 'x_mrpc2']},\n",
      "                      {'class_name': 'input_splitter',\n",
      "                       'in': ['x_rte'],\n",
      "                       'keys_to_extract': [0, 1],\n",
      "                       'out': ['x_rte1', 'x_rte2']},\n",
      "                      {'class_name': 'torch_transformers_preprocessor',\n",
      "                       'in': ['x_mrpc1', 'x_mrpc2'],\n",
      "                       'max_seq_length': 128,\n",
      "                       'out': ['bert_features_mrpc'],\n",
      "                       'vocab_file': 'bert-base-uncased'},\n",
      "                      {'class_name': 'torch_transformers_preprocessor',\n",
      "                       'in': ['x_rte1', 'x_rte2'],\n",
      "                       'max_seq_length': 128,\n",
      "                       'out': ['bert_features_rte'],\n",
      "                       'vocab_file': 'bert-base-uncased'},\n",
      "                      {'class_name': 'simple_vocab',\n",
      "                       'fit_on': ['y_mrpc'],\n",
      "                       'id': 'vocab_mrpc',\n",
      "                       'in': ['y_mrpc'],\n",
      "                       'load_path': '{MODELS_PATH}/mrpc.dict',\n",
      "                       'out': ['y_ids_mrpc'],\n",
      "                       'save_path': '{MODELS_PATH}/mrpc.dict'},\n",
      "                      {'class_name': 'simple_vocab',\n",
      "                       'fit_on': ['y_rte'],\n",
      "                       'id': 'vocab_rte',\n",
      "                       'in': ['y_rte'],\n",
      "                       'load_path': '{MODELS_PATH}/rte.dict',\n",
      "                       'out': ['y_ids_rte'],\n",
      "                       'save_path': '{MODELS_PATH}/rte.dict'},\n",
      "                      {'class_name': 'multitask_pal_bert',\n",
      "                       'gradient_accumulation_steps': '{GRADIENT_ACC_STEPS}',\n",
      "                       'id': 'multitask_pal_bert',\n",
      "                       'in': ['task_id',\n",
      "                              'bert_features_mrpc',\n",
      "                              'bert_features_rte'],\n",
      "                       'in_distribution': {'mrpc': 1, 'rte': 1},\n",
      "                       'in_y': ['y_ids_mrpc', 'y_ids_rte'],\n",
      "                       'in_y_distribution': {'mrpc': 1, 'rte': 1},\n",
      "                       'learning_rate_drop_div': 2.0,\n",
      "                       'learning_rate_drop_patience': 2,\n",
      "                       'load_path': '{MODELS_PATH}/model',\n",
      "                       'optimizer_parameters': {'lr': 4e-05},\n",
      "                       'out': ['y_mrpc_pred_probas', 'y_rte_pred_probas'],\n",
      "                       'pretrained_bert': '{PRETRAINED_BERT}/pytorch_model.bin',\n",
      "                       'return_probas': True,\n",
      "                       'save_path': '{MODELS_PATH}/model',\n",
      "                       'steps_per_epoch': '{STEPS_PER_EPOCH}',\n",
      "                       'tasks': {'mrpc': {'n_classes': '#vocab_mrpc.len'},\n",
      "                                 'rte': {'n_classes': '#vocab_rte.len'}}},\n",
      "                      {'class_name': 'proba2labels',\n",
      "                       'in': ['y_mrpc_pred_probas'],\n",
      "                       'max_proba': True,\n",
      "                       'out': ['y_mrpc_pred_ids']},\n",
      "                      {'in': ['y_mrpc_pred_ids'],\n",
      "                       'out': ['y_mrpc_pred_labels'],\n",
      "                       'ref': 'vocab_mrpc'},\n",
      "                      {'class_name': 'proba2labels',\n",
      "                       'in': ['y_rte_pred_probas'],\n",
      "                       'max_proba': True,\n",
      "                       'out': ['y_rte_pred_ids']},\n",
      "                      {'in': ['y_rte_pred_ids'],\n",
      "                       'out': ['y_rte_pred_labels'],\n",
      "                       'ref': 'vocab_rte'}]},\n",
      " 'dataset_iterator': {'class_name': 'multitask_pal_bert_iterator',\n",
      "                      'gradient_accumulation_steps': '{GRADIENT_ACC_STEPS}',\n",
      "                      'num_train_epochs': '{NUM_TRAIN_EPOCHS}',\n",
      "                      'steps_per_epoch': '{STEPS_PER_EPOCH}',\n",
      "                      'tasks': {'mrpc': {'iterator_class_name': 'basic_classification_iterator',\n",
      "                                         'seed': 12},\n",
      "                                'rte': {'iterator_class_name': 'basic_classification_iterator',\n",
      "                                        'seed': 12}}},\n",
      " 'dataset_reader': {'class_name': 'multitask_reader',\n",
      "                    'data_path': 'null',\n",
      "                    'tasks': {'mrpc': {'data_path': '{GLUE_CSV}/MRPC',\n",
      "                                       'reader_class_name': 'basic_classification_reader',\n",
      "                                       'x': ['#1 String', '#2 String'],\n",
      "                                       'y': 'Quality'},\n",
      "                              'rte': {'data_path': '{GLUE_CSV}/RTE',\n",
      "                                      'reader_class_name': 'basic_classification_reader',\n",
      "                                      'x': ['sentence1', 'sentence2'],\n",
      "                                      'y': 'label'}}},\n",
      " 'metadata': {'download': [{'subdir': '{PRETRAINED_BERT}',\n",
      "                            'url': 'https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin'}],\n",
      "              'variables': {'DOWNLOADS_PATH': '{ROOT_PATH}/downloads',\n",
      "                            'GLUE_CSV': '{ROOT_PATH}/glue_csv',\n",
      "                            'GRADIENT_ACC_STEPS': 2,\n",
      "                            'MODELS_PATH': '{ROOT_PATH}/models',\n",
      "                            'NUM_TRAIN_EPOCHS': 10,\n",
      "                            'PRETRAINED_BERT': '{ROOT_PATH}/pretrained_bert',\n",
      "                            'ROOT_PATH': '/content',\n",
      "                            'STEPS_PER_EPOCH': 210}},\n",
      " 'train': {'batch_size': 16,\n",
      "           'class_name': 'torch_trainer',\n",
      "           'epochs': '{NUM_TRAIN_EPOCHS}',\n",
      "           'evaluation_targets': ['valid'],\n",
      "           'log_every_n_epochs': 1,\n",
      "           'metrics': [{'inputs': ['y_ids_rte',\n",
      "                                   'y_ids_mrpc',\n",
      "                                   'y_rte_pred_ids',\n",
      "                                   'y_mrpc_pred_ids'],\n",
      "                        'name': 'multitask_accuracy'},\n",
      "                       {'inputs': ['y_ids_mrpc', 'y_mrpc_pred_ids'],\n",
      "                        'name': 'accuracy'},\n",
      "                       {'inputs': ['y_ids_rte', 'y_rte_pred_ids'],\n",
      "                        'name': 'f1'}],\n",
      "           'show_examples': False,\n",
      "           'val_every_n_epochs': 1,\n",
      "           'validation_patience': 5}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from deeppavlov import configs\n",
    "\n",
    "train_config = json.load(open(configs.tutorials.multitask_pal_bert.mt_pal_bert_mrpc_rte_tutorial))\n",
    "\n",
    "pprint(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Q5CdXX2WWNr"
   },
   "source": [
    "Now, we can see the the config has a lot of components in the config.\n",
    "Here we will cover the components related to Multi-Task Pal Bert. For the other compnents you may refer the documentation [here](http://docs.deeppavlov.ai/en/master/intro/quick_start.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkMHPwU23yFP"
   },
   "source": [
    "As discussed above to use multitask pal bert we need to make use that we need to use **4 basic components**, which are:\n",
    "\n",
    "\n",
    "- `multitask_reader`\n",
    "\n",
    "- `multitask_pal_iterator`\n",
    "\n",
    "- `multitask_pal_bert_preprocessor`\n",
    "\n",
    "- `multitask_pal_bert`\n",
    "\n",
    "We will explore these components one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7b2qRak5BKn"
   },
   "source": [
    "## Multi-Task Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Io1JndLj3xmH",
    "outputId": "f3d81e75-1443-47b6-c07a-b1acc57d0c36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_name': 'multitask_reader',\n",
      " 'data_path': 'null',\n",
      " 'tasks': {'mrpc': {'data_path': '{GLUE_CSV}/MRPC',\n",
      "                    'reader_class_name': 'basic_classification_reader',\n",
      "                    'x': ['#1 String', '#2 String'],\n",
      "                    'y': 'Quality'},\n",
      "           'rte': {'data_path': '{GLUE_CSV}/RTE',\n",
      "                   'reader_class_name': 'basic_classification_reader',\n",
      "                   'x': ['sentence1', 'sentence2'],\n",
      "                   'y': 'label'}}}\n"
     ]
    }
   ],
   "source": [
    "pprint(train_config[\"dataset_reader\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOsHeBiZ5i-S"
   },
   "source": [
    "`multitask_reader`: As we can see this is a collection of the readers we use for each task. In the tasks dict we have defined the `task_name` along with the all other parameters required by the `reader_class_name`.\n",
    "\n",
    "Here we are using the `basic_classification_reader` to read the data, you can use other readers in DeepPavlov or create your own reader as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLOX3j567mp0"
   },
   "source": [
    "## Multi-Task Pal Bert Iterator\n",
    "\n",
    "This iterator is specific to this pal bert model as it takes care of switching the task while training as explained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E-hsYiFr2CMs",
    "outputId": "4ee0c97a-1d3e-44cc-ce66-e8c5a024871a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_name': 'multitask_pal_bert_iterator',\n",
      " 'gradient_accumulation_steps': '{GRADIENT_ACC_STEPS}',\n",
      " 'num_train_epochs': '{NUM_TRAIN_EPOCHS}',\n",
      " 'steps_per_epoch': '{STEPS_PER_EPOCH}',\n",
      " 'tasks': {'mrpc': {'iterator_class_name': 'basic_classification_iterator',\n",
      "                    'seed': 12},\n",
      "           'rte': {'iterator_class_name': 'basic_classification_iterator',\n",
      "                   'seed': 12}}}\n"
     ]
    }
   ],
   "source": [
    "pprint(train_config[\"dataset_iterator\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEJFni0R80q6"
   },
   "source": [
    "`multitask_pal_bert_iterator`: We can see this is also a collection for other iterators present in DeepPavlov but it also required some other parameters:\n",
    "\n",
    "  - `num_train_epochs`: Total number to training epochs as this is also used for task selection during training.\n",
    "\n",
    "  - `steps_per_epoch`: Number of steps taken per epoch, this is also used for task selection during training.\n",
    "\n",
    "  - `gradient_accumulation_steps`: Number of gradient accmulation steps. This is required because we train on the same task for the number of gradient accumulation steps.\n",
    "\n",
    "  - `tasks`: This is similar to the dict we have the the multitask_reader just the readers are replaced with iterators and the parameters for that iterator should be provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5bG3d5F_4Ug"
   },
   "source": [
    "## Multi-Task Pal Bert Precrocessor\n",
    "\n",
    "This is used to extract the `task_id` form the inputs for each task, later we would need to pass this `task_id` in the `multitask_pal_bert` component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rTq8KMZ2CDm",
    "outputId": "855ff3b2-0972-406e-efb0-f112c2c4f5dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_name': 'multitask_pal_bert_preprocessor',\n",
      " 'in': ['x_mrpc_with_id', 'x_rte_with_id'],\n",
      " 'out': ['task_id', 'x_mrpc', 'x_rte']}\n"
     ]
    }
   ],
   "source": [
    "pprint(train_config[\"chainer\"][\"pipe\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNrB3sS1BA2e"
   },
   "source": [
    "## The Model - Multi-Task Pal Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_zXXUgI2Bzr",
    "outputId": "70e16ee4-2b89-42c5-bcf5-47db226a36ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_name': 'multitask_pal_bert',\n",
      " 'gradient_accumulation_steps': '{GRADIENT_ACC_STEPS}',\n",
      " 'id': 'multitask_pal_bert',\n",
      " 'in': ['task_id', 'bert_features_mrpc', 'bert_features_rte'],\n",
      " 'in_distribution': {'mrpc': 1, 'rte': 1},\n",
      " 'in_y': ['y_ids_mrpc', 'y_ids_rte'],\n",
      " 'in_y_distribution': {'mrpc': 1, 'rte': 1},\n",
      " 'learning_rate_drop_div': 2.0,\n",
      " 'learning_rate_drop_patience': 2,\n",
      " 'load_path': '{MODELS_PATH}/model',\n",
      " 'optimizer_parameters': {'lr': 4e-05},\n",
      " 'out': ['y_mrpc_pred_probas', 'y_rte_pred_probas'],\n",
      " 'pretrained_bert': '{PRETRAINED_BERT}/pytorch_model.bin',\n",
      " 'return_probas': True,\n",
      " 'save_path': '{MODELS_PATH}/model',\n",
      " 'steps_per_epoch': '{STEPS_PER_EPOCH}',\n",
      " 'tasks': {'mrpc': {'n_classes': '#vocab_mrpc.len'},\n",
      "           'rte': {'n_classes': '#vocab_rte.len'}}}\n"
     ]
    }
   ],
   "source": [
    "pprint(train_config[\"chainer\"][\"pipe\"][7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWbrr6aeBh8K"
   },
   "source": [
    "`multitask_pal_bert`: The is the model as in the paper Bert-n-Pals along with the other function to load, train and predict. Parameters used:\n",
    "\n",
    "\n",
    "  - `in`: This is the input to the component. When using the `multitask_pal_bert` the first input should be `task_id` which we extracted using the `multitask_pal_bert_preprocessor`.\n",
    "\n",
    "  - `in_distribution`: This is a dict that contains the number of input parameters that would be needed for each task, since these two are classification tasks we would only require 1 input feature for each task.\n",
    "\n",
    "  -  `in_y_distribution`: This is similar to `in_distribution` but for the labels.\n",
    "\n",
    "  - `num_train_epochs`: Total number to training epochs as this is also used for task selection during training.\n",
    "\n",
    "  - `steps_per_epoch`: Number of steps taken per epoch, this is also used for task selection during training.\n",
    "\n",
    "  - `gradient_accumulation_steps`: Number of gradient accmulation steps. This is required because we train on the same task for the number of gradient accumulation steps.\n",
    "\n",
    "  - `pretrained_bert`: Path to the pretrained bert-base-uncased pytorch model from hugging face.\n",
    "  \n",
    "  - `tasks`: dict of task names with\n",
    "    - `n_classes`: Number of prediction classes for the task.\n",
    "    - `task_type`: Defaults to classification. Can also be set to `\"regression\"` if the task is a regression task. \n",
    " \n",
    " All other parameters are not precific to this model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTexzs2OIQub"
   },
   "source": [
    "## Before Training - Install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvrNlt4VBL_k",
    "outputId": "0e4358d4-9dea-49c9-ea2f-b4acb39f72bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-19 11:34:25.892 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'mt_pal_bert_mrpc_rte_tutorial' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/tutorials/multitask_pal_bert/mt_pal_bert_mrpc_rte_tutorial.json'\n",
      "Collecting torch==1.6.0\n",
      "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 748.8 MB 19 kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.18.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0+cu102\n",
      "    Uninstalling torch-1.9.0+cu102:\n",
      "      Successfully uninstalled torch-1.9.0+cu102\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.6.0 which is incompatible.\n",
      "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.6.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.6.0\n",
      "Collecting torchvision==0.7.0\n",
      "  Downloading torchvision-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 3.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (1.6.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (7.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (1.18.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->torchvision==0.7.0) (0.16.0)\n",
      "Installing collected packages: torchvision\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.10.0+cu102\n",
      "    Uninstalling torchvision-0.10.0+cu102:\n",
      "      Successfully uninstalled torchvision-0.10.0+cu102\n",
      "Successfully installed torchvision-0.7.0\n",
      "Collecting tensorflow==1.15.5\n",
      "  Downloading tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 110.5 MB 548 bytes/s \n",
      "\u001b[?25hRequirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.18.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.17.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.12.0)\n",
      "Collecting tensorflow-estimator==1.15.1\n",
      "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[K     |████████████████████████████████| 503 kB 45.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.8.1)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0\n",
      "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 33.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: h5py<=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (2.10.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.39.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.37.0)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 6.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.3.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (57.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.6.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.5.0)\n",
      "Building wheels for collected packages: gast\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=bf500666e1a939bcdc5dda11c1ff8a7dd1055887be9f4cc54057cd5513e860ab\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
      "Successfully built gast\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.6.0\n",
      "    Uninstalling tensorflow-estimator-2.6.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.6.0\n",
      "    Uninstalling tensorboard-2.6.0:\n",
      "      Successfully uninstalled tensorboard-2.6.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.4.0\n",
      "    Uninstalling gast-0.4.0:\n",
      "      Successfully uninstalled gast-0.4.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.6.0\n",
      "    Uninstalling tensorflow-2.6.0:\n",
      "      Successfully uninstalled tensorflow-2.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-probability 0.13.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
      "kapre 0.3.5 requires numpy>=1.18.5, but you have numpy 1.18.0 which is incompatible.\n",
      "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.5 which is incompatible.\u001b[0m\n",
      "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.5 tensorflow-estimator-1.15.1\n",
      "Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
      "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-wekf80b2\n",
      "  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-wekf80b2\n",
      "Building wheels for collected packages: bert-dp\n",
      "  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for bert-dp: filename=bert_dp-1.0-py3-none-any.whl size=23591 sha256=6caf4f4580b2b8a56f1dc438646be73b2cf6f9042e003293dd8771bb40f87e50\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tl74x18p/wheels/44/29/b2/ee614cb7f97ba5c2d220029eaede3af4b74331ad31d6e2f4eb\n",
      "Successfully built bert-dp\n",
      "Installing collected packages: bert-dp\n",
      "Successfully installed bert-dp-1.0\n",
      "Collecting transformers==4.6.0\n",
      "  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 4.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (21.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 18.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (4.41.1)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (1.18.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (2.22.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (4.6.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (2019.12.20)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (0.0.35)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.0) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.0) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0) (2.4.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (2.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (1.15.0)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.0.8 tokenizers-0.10.3 transformers-4.6.0\n"
     ]
    }
   ],
   "source": [
    "! python -m deeppavlov install mt_pal_bert_mrpc_rte_tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK0LsA0KJ6en"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "We set `download=True` as we a training the model from stratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbnCkjOHmGu-",
    "outputId": "0ce79059-1834-4143-f0f5-5b59c2e6c333"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 11:59:33.172 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin to /content/pretrained_bert/pytorch_model.bin\n",
      "100%|██████████| 440M/440M [00:16<00:00, 26.9MB/s]\n",
      "2021-08-19 11:59:51.340 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 112: Cannot find /content/glue_csv/MRPC/test.csv file\n",
      "2021-08-19 11:59:51.758 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 112: Cannot find /content/glue_csv/RTE/test.csv file\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "2021-08-19 12:00:02.173 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/models/mrpc.dict]\n",
      "2021-08-19 12:00:02.186 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /content/models/mrpc.dict]\n",
      "2021-08-19 12:00:02.189 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/models/rte.dict]\n",
      "2021-08-19 12:00:02.282 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /content/models/rte.dict]\n",
      "2021-08-19 12:00:02.317 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 210: Load path /content/models/model is given.\n",
      "2021-08-19 12:00:02.318 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 219: Load path /content/models/model.pth.tar exists.\n",
      "2021-08-19 12:00:02.322 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 221: Initializing `MultiTaskPalBert` from saved.\n",
      "2021-08-19 12:00:07.458 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 166: Bert Model Weights Loaded.\n",
      "2021-08-19 12:00:07.474 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 228: Loading weights from /content/models/model.pth.tar.\n",
      "2021-08-19 12:00:07.809 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 102: Model was successfully initialized!, log model summary is set to false\n",
      "2021-08-19 12:01:15.112 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 199: Initial best multitask_accuracy of 0.1585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1741, \"metrics\": {\"multitask_accuracy\": 0.1585, \"accuracy\": 0.3354, \"f1\": 0.642}, \"time_spent\": \"0:01:08\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 16, \"metrics\": {\"multitask_accuracy\": 0.5625, \"accuracy\": 0.625, \"f1\": 0.7143}, \"time_spent\": \"0:04:21\", \"epochs_done\": 1, \"batches_seen\": 210, \"train_examples_seen\": 3360, \"losses\": [0.23631954193115234, 0.3407200574874878]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 12:05:35.6 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best multitask_accuracy of 0.3688\n",
      "2021-08-19 12:05:35.7 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
      "2021-08-19 12:05:35.9 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /content/models/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1741, \"metrics\": {\"multitask_accuracy\": 0.3688, \"accuracy\": 0.7122, \"f1\": 0.2456}, \"time_spent\": \"0:05:28\", \"epochs_done\": 1, \"batches_seen\": 210, \"train_examples_seen\": 3360, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 16, \"metrics\": {\"multitask_accuracy\": 0.75, \"accuracy\": 0.8125, \"f1\": 0.75}, \"time_spent\": \"0:08:46\", \"epochs_done\": 2, \"batches_seen\": 420, \"train_examples_seen\": 6720, \"losses\": [0.15237532556056976, 0.2099573314189911]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 12:10:00.591 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best multitask_accuracy of 0.4848\n",
      "2021-08-19 12:10:00.593 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
      "2021-08-19 12:10:00.599 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /content/models/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1741, \"metrics\": {\"multitask_accuracy\": 0.4848, \"accuracy\": 0.7679, \"f1\": 0.5931}, \"time_spent\": \"0:09:53\", \"epochs_done\": 2, \"batches_seen\": 420, \"train_examples_seen\": 6720, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 16, \"metrics\": {\"multitask_accuracy\": 0.6875, \"accuracy\": 0.8125, \"f1\": 0.8}, \"time_spent\": \"0:13:12\", \"epochs_done\": 3, \"batches_seen\": 630, \"train_examples_seen\": 10080, \"losses\": [0.06324110180139542, 0.303523987531662]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 12:14:26.137 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best multitask_accuracy of 0.5221\n",
      "2021-08-19 12:14:26.139 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
      "2021-08-19 12:14:26.145 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /content/models/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1741, \"metrics\": {\"multitask_accuracy\": 0.5221, \"accuracy\": 0.7829, \"f1\": 0.6476}, \"time_spent\": \"0:14:19\", \"epochs_done\": 3, \"batches_seen\": 630, \"train_examples_seen\": 10080, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 16, \"metrics\": {\"multitask_accuracy\": 0.8125, \"accuracy\": 0.875, \"f1\": 0.9565}, \"time_spent\": \"0:17:37\", \"epochs_done\": 4, \"batches_seen\": 840, \"train_examples_seen\": 13440, \"losses\": [0.09093309193849564, 0.2428038865327835]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 12:18:51.609 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best multitask_accuracy of 0.529\n",
      "2021-08-19 12:18:51.611 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
      "2021-08-19 12:18:51.616 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /content/models/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1741, \"metrics\": {\"multitask_accuracy\": 0.529, \"accuracy\": 0.7938, \"f1\": 0.6623}, \"time_spent\": \"0:18:44\", \"epochs_done\": 4, \"batches_seen\": 840, \"train_examples_seen\": 13440, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 16, \"metrics\": {\"multitask_accuracy\": 0.9375, \"accuracy\": 1.0, \"f1\": 0.9333}, \"time_spent\": \"0:22:02\", \"epochs_done\": 5, \"batches_seen\": 1050, \"train_examples_seen\": 16800, \"losses\": [0.02166750468313694, 0.09673721343278885]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 12:23:16.57 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best multitask_accuracy of 0.5692\n",
      "2021-08-19 12:23:16.58 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
      "2021-08-19 12:23:16.60 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /content/models/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1741, \"metrics\": {\"multitask_accuracy\": 0.5692, \"accuracy\": 0.8277, \"f1\": 0.6853}, \"time_spent\": \"0:23:09\", \"epochs_done\": 5, \"batches_seen\": 1050, \"train_examples_seen\": 16800, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 16, \"metrics\": {\"multitask_accuracy\": 0.9375, \"accuracy\": 1.0, \"f1\": 0.9474}, \"time_spent\": \"0:26:27\", \"epochs_done\": 6, \"batches_seen\": 1260, \"train_examples_seen\": 20160, \"losses\": [0.06900206953287125, 0.062497738748788834]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 12:27:41.168 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best multitask_accuracy of 0.5704\n",
      "2021-08-19 12:27:41.170 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
      "2021-08-19 12:27:41.179 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /content/models/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1741, \"metrics\": {\"multitask_accuracy\": 0.5704, \"accuracy\": 0.8346, \"f1\": 0.676}, \"time_spent\": \"0:27:34\", \"epochs_done\": 6, \"batches_seen\": 1260, \"train_examples_seen\": 20160, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 16, \"metrics\": {\"multitask_accuracy\": 0.875, \"accuracy\": 1.0, \"f1\": 0.875}, \"time_spent\": \"0:30:52\", \"epochs_done\": 7, \"batches_seen\": 1470, \"train_examples_seen\": 23520, \"losses\": [0.05802572891116142, 0.04223836585879326]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 12:32:05.739 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the multitask_accuracy of 0.5704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1741, \"metrics\": {\"multitask_accuracy\": 0.5256, \"accuracy\": 0.8179, \"f1\": 0.6799}, \"time_spent\": \"0:31:58\", \"epochs_done\": 7, \"batches_seen\": 1470, \"train_examples_seen\": 23520, \"impatience\": 1, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 16, \"metrics\": {\"multitask_accuracy\": 1.0, \"accuracy\": 1.0, \"f1\": 1.0}, \"time_spent\": \"0:35:10\", \"epochs_done\": 8, \"batches_seen\": 1680, \"train_examples_seen\": 26880, \"losses\": [0.0017701720353215933, 0.01832728646695614]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 12:36:24.434 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the multitask_accuracy of 0.5704\n",
      "2021-08-19 12:36:24.436 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 223: ----------Current LR is decreased in 2.0 times----------\n",
      "2021-08-19 12:36:24.439 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 210: Load path /content/models/model is given.\n",
      "2021-08-19 12:36:24.446 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 219: Load path /content/models/model.pth.tar exists.\n",
      "2021-08-19 12:36:24.449 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 221: Initializing `MultiTaskPalBert` from saved.\n",
      "2021-08-19 12:36:39.799 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 166: Bert Model Weights Loaded.\n",
      "2021-08-19 12:36:39.813 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 228: Loading weights from /content/models/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1741, \"metrics\": {\"multitask_accuracy\": 0.5238, \"accuracy\": 0.8041, \"f1\": 0.6015}, \"time_spent\": \"0:36:17\", \"epochs_done\": 8, \"batches_seen\": 1680, \"train_examples_seen\": 26880, \"impatience\": 2, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 16, \"metrics\": {\"multitask_accuracy\": 1.0, \"accuracy\": 1.0, \"f1\": 1.0}, \"time_spent\": \"0:39:43\", \"epochs_done\": 9, \"batches_seen\": 1890, \"train_examples_seen\": 30240, \"losses\": [0.060667600482702255, 0.018913909792900085]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 12:40:57.218 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best multitask_accuracy of 0.5767\n",
      "2021-08-19 12:40:57.220 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
      "2021-08-19 12:40:57.221 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /content/models/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1741, \"metrics\": {\"multitask_accuracy\": 0.5767, \"accuracy\": 0.834, \"f1\": 0.6844}, \"time_spent\": \"0:40:50\", \"epochs_done\": 9, \"batches_seen\": 1890, \"train_examples_seen\": 30240, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 16, \"metrics\": {\"multitask_accuracy\": 1.0, \"accuracy\": 1.0, \"f1\": 1.0}, \"time_spent\": \"0:44:08\", \"epochs_done\": 10, \"batches_seen\": 2100, \"train_examples_seen\": 33600, \"losses\": [0.0013259320985525846, 0.01645125448703766]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 12:45:21.913 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the multitask_accuracy of 0.5767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1741, \"metrics\": {\"multitask_accuracy\": 0.5681, \"accuracy\": 0.8283, \"f1\": 0.6567}, \"time_spent\": \"0:45:15\", \"epochs_done\": 10, \"batches_seen\": 2100, \"train_examples_seen\": 33600, \"impatience\": 1, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 12:45:31.149 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/models/mrpc.dict]\n",
      "2021-08-19 12:45:31.169 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/models/rte.dict]\n",
      "2021-08-19 12:45:31.173 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 210: Load path /content/models/model is given.\n",
      "2021-08-19 12:45:31.176 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 219: Load path /content/models/model.pth.tar exists.\n",
      "2021-08-19 12:45:31.179 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 221: Initializing `MultiTaskPalBert` from saved.\n",
      "2021-08-19 12:45:34.579 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 166: Bert Model Weights Loaded.\n",
      "2021-08-19 12:45:34.598 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 228: Loading weights from /content/models/model.pth.tar.\n",
      "2021-08-19 12:45:35.483 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 102: Model was successfully initialized!, log model summary is set to false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1741, \"metrics\": {\"multitask_accuracy\": 0.5767, \"accuracy\": 0.834, \"f1\": 0.6844}, \"time_spent\": \"0:01:07\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 12:46:51.568 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/models/mrpc.dict]\n",
      "2021-08-19 12:46:51.571 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/models/rte.dict]\n",
      "2021-08-19 12:46:51.577 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 210: Load path /content/models/model is given.\n",
      "2021-08-19 12:46:51.580 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 219: Load path /content/models/model.pth.tar exists.\n",
      "2021-08-19 12:46:51.588 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 221: Initializing `MultiTaskPalBert` from saved.\n",
      "2021-08-19 12:46:54.913 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 166: Bert Model Weights Loaded.\n",
      "2021-08-19 12:46:54.926 INFO in 'deeppavlov.models.multitask_pal_bert.multitask_pal_bert'['multitask_pal_bert'] at line 228: Loading weights from /content/models/model.pth.tar.\n",
      "2021-08-19 12:46:55.843 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 102: Model was successfully initialized!, log model summary is set to false\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import train_model\n",
    "\n",
    "model = train_model(train_config, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPB7lNgQTAM7"
   },
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VG75dgtoJTFF",
    "outputId": "73cf67e8-7b77-4fec-8aa5-2b8a5e9c5521"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1'], ['entailment']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([[-1, [\"The increase reflects lower credit losses and favorable interest rates.\", \"The gain came as a result of fewer credit losses and lower interest rates.\"]]], \n",
    "      [[-1, [\"Mount Olympus towers up from the center of the earth.\", \"Mount Olympus is in the center of the earth.\"]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aueJHHCoJp1E",
    "outputId": "5e42e524-b54e-46d5-96b6-1f4cf90596cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0'], ['not_entailment']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([[-1, [\"The increase reflects lower credit losses and favorable interest rates.\", \"Mount Olympus is in the center of the earth.\"]]], \n",
    "      [[-1, [\"Mount Olympus towers up from the center of the earth.\", \"The gain came as a result of fewer credit losses and lower interest rates.\"]]])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mt_pal bert_tutorial_mrpc_rte.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
